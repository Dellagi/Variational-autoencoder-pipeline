{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KafkaServerIP = dbutils.widgets.get(\"ip\")\n",
    "KafkaServerPort = dbutils.widgets.get(\"port\")\n",
    "srcTopicName = dbutils.widgets.get(\"topicname\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import *\n",
    "import struct\n",
    "import numpy as np\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c6fdc5da-e332-4594-a412-ed3ba74771bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dfraw = spark \\\n",
    "            .readStream \\\n",
    "            .format(\"kafka\") \\\n",
    "            .option(\"kafka.bootstrap.servers\", f'{KafkaServerIP}:{KafkaServerPort}') \\\n",
    "            .option(\"subscribe\", f'{srcTopicName}') \\\n",
    "            .load()\n",
    "\n",
    "df = dfraw.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "\n",
    "kafka_df = df.selectExpr(\"CAST(value AS STRING)\") \\\n",
    "\n",
    "output_query = kafka_df.writeStream \\\n",
    "                      .queryName(\"storeKafka\") \\\n",
    "                      .format(\"memory\") \\\n",
    "                      .start()\n",
    "output_query.awaitTermination(10)\n",
    "\n",
    "sample_df = spark.sql(\"select * from storeKafka\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4ef414f5-ab18-4101-8c20-0d7c1b828884",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import *\n",
    "import struct\n",
    "import numpy as np\n",
    "\n",
    "def b64_to_arr(value):\n",
    "    image = Image.open(io.BytesIO(base64.b64decode(value)))\n",
    "    image_np = np.array(image)\n",
    "    return (image_np.tolist(), )\n",
    "\n",
    "\n",
    "m_schema =  StructType([\n",
    "  StructField('matrix', ArrayType(ArrayType(IntegerType()))), \n",
    "  ])\n",
    "\n",
    "b64_to_arr_udf = f.udf(b64_to_arr, m_schema)\n",
    "\n",
    "df = spark \\\n",
    "            .readStream \\\n",
    "            .format(\"kafka\") \\\n",
    "            .option(\"kafka.bootstrap.servers\", f'{KafkaServerIP}:{KafkaServerPort}') \\\n",
    "            .option(\"subscribe\", f'{srcTopicName}') \\\n",
    "            .load() \\\n",
    "            .selectExpr(\"CAST(value AS STRING)\") \\\n",
    "            .withColumn('decoded', b64_to_arr_udf(f.col(\"value\"))) \\\n",
    "            .select(\"decoded\")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d61f01f1-519f-44f4-b8b5-ac93d64e53ef",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def poolingOverlap(value, f=2, stride=None, method='max', pad=False, return_max_pos=False):\n",
    "    image = Image.open(io.BytesIO(base64.b64decode(value)))\n",
    "    mat = np.array(image)\n",
    "    m, n = mat.shape[:2]\n",
    "    if stride is None:\n",
    "        stride = f\n",
    "    _ceil = lambda x, y: x//y + 1\n",
    "    if pad:\n",
    "        ny = _ceil(m, stride)\n",
    "        nx = _ceil(n, stride)\n",
    "        size = ((ny-1)*stride+f, (nx-1)*stride+f) + mat.shape[2:]\n",
    "        mat_pad = np.full(size, 0)\n",
    "        mat_pad[:m, :n, ...] = mat\n",
    "    else:\n",
    "        mat_pad = mat[:(m-f)//stride*stride+f, :(n-f)//stride*stride+f, ...]\n",
    "\n",
    "    s0, s1 = mat_pad.strides[:2]\n",
    "    m1, n1 = mat_pad.shape[:2]\n",
    "    m2, n2 = (f, f)[:2]\n",
    "    view_shape = (1+(m1-m2)//stride, 1+(n1-n2)//stride, m2, n2)+mat_pad.shape[2:]\n",
    "    strides = (stride*s0, stride*s1, s0, s1)+mat_pad.strides[2:]\n",
    "    view = np.lib.stride_tricks.as_strided(mat_pad, view_shape, strides=strides, writeable=False)\n",
    "\n",
    "    if method == 'max':\n",
    "        result = np.nanmax(view, axis=(2, 3), keepdims=return_max_pos)\n",
    "    else:\n",
    "        result = np.nanmean(view, axis=(2, 3), keepdims=return_max_pos)\n",
    "    if return_max_pos:\n",
    "        pos = np.where(result == view, 1, 0)\n",
    "        result = np.squeeze(result)\n",
    "        return result.tolist(), [[pos]]\n",
    "    else:\n",
    "        return mat.tolist(), result.tolist()\n",
    "\n",
    "\n",
    "m_schema =  StructType([\n",
    "  StructField('matrix', ArrayType(ArrayType(IntegerType()))), \\\n",
    "  StructField('max_pool_matrix', ArrayType(ArrayType(IntegerType())))     \n",
    "  ])\n",
    "\n",
    "poolingOverlap_udf = f.udf(poolingOverlap, m_schema)\n",
    "\n",
    "df = spark \\\n",
    "            .readStream \\\n",
    "            .format(\"kafka\") \\\n",
    "            .option(\"kafka.bootstrap.servers\", \"65.20.115.103:9092\") \\\n",
    "            .option(\"subscribe\", \"spark\") \\\n",
    "            .load() \\\n",
    "            .selectExpr(\"CAST(value AS STRING)\") \\\n",
    "            .withColumn('parsed', poolingOverlap_udf(f.col(\"value\"))) \\\n",
    "            .select(\"parsed.matrix\",\"parsed.max_pool_matrix\")\n",
    "\n",
    "\n",
    "df.writeStream \\\n",
    "  .queryName(\"storeKafka\") \\\n",
    "  .format(\"memory\") \\\n",
    "  .start()\n",
    "\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "readKafka",
   "notebookOrigID": 1859167485983405,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
